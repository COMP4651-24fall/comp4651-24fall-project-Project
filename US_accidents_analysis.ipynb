{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2d74a8-e8a4-466d-9e1b-2f5025e37b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5a5ace-9f35-44f0-bf13-d851d30564cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LabbKLYBZV72",
    "outputId": "979e66e0-b9e4-4024-a071-4f31e569af62"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sobhanmoosavi/us-accidents\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0257949e-8af1-4244-a830-611c3417270a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dCpQqVa3ZeCN"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.rdd import RDD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec35e98-2e37-405d-ac4b-139e79882469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "E1Qhy4j_ZzW2"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"COMP4651Project\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128mb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv('file:' + path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ef6e03-0c16-4d96-a78a-bef0ed948d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "5Nym-1HWPvMf",
    "outputId": "15fe31b0-c13f-4935-8316-5a18c64788d2"
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e846954b-e37e-419e-95d3-8f5fbd5f169c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wdXN-nER4WA",
    "outputId": "830f1b1a-ce0f-4ece-8fbb-79b14040ea74"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c54ef8a-0615-411e-9c75-3a64bc8a3a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LHUkjvw8WNa5"
   },
   "source": [
    "# show NA values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45db0b96-100f-488a-8fa4-9d1a4c27a28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7xo5EJRWPFZ",
    "outputId": "12bcfb2b-30d9-4b4c-dca1-0a2f0d462e6f"
   },
   "outputs": [],
   "source": [
    "df.na.drop(how=\"any\", thresh=2).count()\n",
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de138d9-e154-44f8-be53-4ae63c793c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EJ4kHVVvhXpk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5a34d7-8f6a-4de7-933f-1afaa1bf2014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lFU9OEwgjEa",
    "outputId": "a5e0ee1a-1f1c-4340-8512-3a3bdcb538f7"
   },
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ce2ef0-1477-4a59-b7b9-736b2e1e37d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HbODPQJYjQC1",
    "outputId": "1ab335ab-3b4d-42b9-9d82-78abe7af03b3"
   },
   "outputs": [],
   "source": [
    "first_element = rdd.take(1)\n",
    "first_element[0].ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2057ab-9b9b-48e8-ad96-36a2749fe850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VEjTWFqaHvBO"
   },
   "source": [
    "# Total cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63bbf1ba-e3e2-4e3f-b757-36c4a090b30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ho53S7EzHyOe",
    "outputId": "e65a960b-a5e0-4127-e910-11c0538ac490"
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame to an RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Map each row to a count of 1\n",
    "mapped_rdd = rdd.map(lambda x: 1)\n",
    "\n",
    "# Reduce by key to sum up all the counts\n",
    "total_cases = mapped_rdd.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(\"Total number of cases:\", total_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66623fe2-ebc6-41da-9090-7a468b34c45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Y_AWIBr_pmkI"
   },
   "source": [
    "# Count by Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8705290-974c-4bc6-bbb9-81aa97e0845a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aAZ8IoF4jeaL"
   },
   "outputs": [],
   "source": [
    "mapped_rdd = rdd.map(lambda x: (x.City, 1))\n",
    "reduced_rdd = mapped_rdd.reduceByKey(lambda a, b: a + b)\n",
    "result_df = reduced_rdd.toDF([\"City\", \"Cases\"])\n",
    "result_desc_df = result_df.orderBy(col(\"Cases\").desc()).limit(15)\n",
    "result_desc_df = result_desc_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c2b8c1-0cb5-46ca-a16d-4844741700da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eDYrZ2h_SRky"
   },
   "outputs": [],
   "source": [
    "result_df_imporved = df.groupBy(\"City\").agg(count(\"*\").alias(\"Cases\"))\n",
    "result_desc_df = result_df_imporved.orderBy(col(\"Cases\").desc()).limit(15)\n",
    "result_desc_df = result_desc_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cef4ae-7765-4c60-b57b-e2c63583744f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "AJ3Ez2vkhRdO",
    "outputId": "a1d05647-d5f0-4d0d-a66f-669e713cd14a"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(result_desc_df[\"City\"], result_desc_df[\"Cases\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.title(\"Top 15 Cities by Number of Accidents\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the city with the most accidents on top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831985c2-c3a9-46bd-a4ba-1da83c6cea9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hrUAvGO-rHsu"
   },
   "outputs": [],
   "source": [
    "result_asec_df = result_df_imporved.orderBy(col(\"Cases\")).limit(15)\n",
    "result_asec_df = result_asec_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbbb90c-0473-4e0b-b31e-143a36975b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "um3wF36MLl7N",
    "outputId": "761f9049-44cf-43d2-9d1b-1e1aa8b83a8d"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(result_asec_df[\"City\"], result_asec_df[\"Cases\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.title(\"Least 15 Cities by Number of Accidents\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the city with the most accidents on top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45fd4f07-4b14-40d9-9146-264614dd123c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KVbe5fioX_5a"
   },
   "source": [
    "Find the total number of cities in result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7200a05e-a00d-4c3c-8eb5-97e14125810e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EYay-4VYDVc",
    "outputId": "248b4ac6-4a81-45a0-87e1-377339c8adcc"
   },
   "outputs": [],
   "source": [
    "num_cities = result_df_imporved.count()\n",
    "print(f\"Total number of cities in result_df_improved: {num_cities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c7f891-19f4-4290-84ca-454bdfcaa5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "m8rSwoHVX53f"
   },
   "source": [
    "Find the total number of accident cases in these 15 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eafe94c-629b-4f74-80f5-50aaf637e3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYBFvfN3YMUc",
    "outputId": "adf95a57-483f-48e2-d905-4f90d23f3ff0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sum_of_cases = result_desc_df['Cases'].sum()\n",
    "\n",
    "print(f\"The sum of cases in the top 15 cities is: {sum_of_cases}\")\n",
    "print(f\"percentage :  {sum_of_cases / total_cases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae8dc7e-ecf4-4fc8-8290-4af252fab2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZenTO_g2pz8Y"
   },
   "source": [
    "# Count By States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "badb31ac-54db-4a6c-afca-1de690060125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "w7ceXiyJpgdY"
   },
   "outputs": [],
   "source": [
    "mapped_state_rdd = rdd.map(lambda x: (x.State, 1))\n",
    "reduced_rdd = mapped_state_rdd.reduceByKey(lambda a, b: a + b)\n",
    "result_df = reduced_rdd.toDF([\"State\", \"Cases\"])\n",
    "\n",
    "result_desc_df = result_df.orderBy(col(\"Cases\").desc()).limit(15)\n",
    "result_desc_df = result_desc_df.toPandas()\n",
    "\n",
    "result_asec_df = result_df.orderBy(col(\"Cases\")).limit(15)\n",
    "result_asec_df = result_asec_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4194686e-3616-44aa-b6eb-f3bd6fb3dab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "hr8Ro2oMG5ts",
    "outputId": "a3b5a9b6-9416-458d-80dd-4ec52a7383d4"
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of accidents\n",
    "total_accidents = 7728394\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(result_desc_df[\"State\"], result_desc_df[\"Cases\"], color=\"#87CEEB\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.title(\"Top 15 States by Number of Accidents\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add some grid lines\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Add some padding to the y-axis labels\n",
    "plt.gca().tick_params(axis=\"y\", pad=10)\n",
    "\n",
    "# Show the values next to each bar\n",
    "for i, (value, state) in enumerate(zip(result_desc_df[\"Cases\"], result_desc_df[\"State\"])):\n",
    "    percentage = (value / total_accidents) * 100\n",
    "    plt.text(value, i, f\"{value:,} ({percentage:.2f}%)\", va=\"center\", ha=\"left\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b36ed8c-18fd-4604-a297-a91aaa40db1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "QZxkRfESMkUl",
    "outputId": "f5b80ae1-ae87-4664-b032-b032f31bfa0b"
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of accidents\n",
    "total_accidents = total_cases\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(result_asec_df[\"State\"], result_asec_df[\"Cases\"], color=\"#87CEEB\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.title(\"Top 15 States with least no. of Accidents\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add some grid lines\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Add some padding to the y-axis labels\n",
    "plt.gca().tick_params(axis=\"y\", pad=10)\n",
    "\n",
    "# Show the values next to each bar\n",
    "for i, (value, state) in enumerate(zip(result_asec_df[\"Cases\"], result_asec_df[\"State\"])):\n",
    "    percentage = (value / total_accidents) * 100\n",
    "    plt.text(value, i, f\"{value:,} ({percentage:.2f}%)\", va=\"center\", ha=\"left\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6c6e91-91b9-4ed0-9a3d-3a719af90f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xUsqg8laQxGw"
   },
   "source": [
    "## visualize by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033dbc88-b79f-4ed4-9895-584c0901731b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbdf8b9-da38-4005-a5ed-06f61dce6b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "WPmyt3_LO26S",
    "outputId": "79ae474d-d6cc-4407-a92e-007c668b3bc4"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Step 1: Prepare your data using PySpark\n",
    "# Assuming df is your original DataFrame with 'Start_Lat', 'Start_Lng', and 'State'\n",
    "# Filter or sample the data if necessary\n",
    "df_sampled = df.select(\"Start_Lat\", \"Start_Lng\").dropna()\n",
    "\n",
    "# Optionally, sample the data for better performance if the dataset is very large\n",
    "df_sampled = df_sampled.sample(withReplacement=False, fraction=400000 / df.count())\n",
    "\n",
    "# Convert to RDD and map to coordinate pairs\n",
    "rdd = df_sampled.rdd.map(lambda row: (row[\"Start_Lat\"], row[\"Start_Lng\"]))\n",
    "\n",
    "# Collect the data back to the driver\n",
    "heat_data = rdd.collect()\n",
    "\n",
    "# Step 2: Create a base map\n",
    "# Center the map on the mean latitude and longitude of the dataset\n",
    "center_lat = df_sampled.selectExpr(\"avg(Start_Lat)\").collect()[0][0]\n",
    "center_lng = df_sampled.selectExpr(\"avg(Start_Lng)\").collect()[0][0]\n",
    "m = folium.Map(location=[center_lat, center_lng], zoom_start=5)\n",
    "\n",
    "# Step 3: Add the heatmap layer\n",
    "HeatMap(heat_data, radius=10, blur=15, max_zoom=1).add_to(m)\n",
    "\n",
    "# Step 4: Display and save the map\n",
    "# m.save(\"accidents_heatmap.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f90893-bdfb-4d5c-9b75-0069b786a257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H_XP2RLFNKBj"
   },
   "source": [
    "# Timezone Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3881e9c4-6c8e-4adf-b1ec-df32ed2b66cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0fni6M5tNLGR"
   },
   "outputs": [],
   "source": [
    "# slow rdd methods\n",
    "# rdd = df.rdd\n",
    "# mapped_rdd = rdd.map(lambda x: (x.Timezone, 1))\n",
    "# # Use combineByKey for optimized aggregation\n",
    "\n",
    "# combined_rdd = mapped_rdd.combineByKey(\n",
    "#     lambda value: value,  # CreateCombiner: Initialize the count for a key\n",
    "#     lambda acc, value: acc + value,  # MergeValue: Add one to the existing count\n",
    "#     lambda acc1, acc2: acc1 + acc2  # MergeCombiners: Combine counts from different partitions\n",
    "# )\n",
    "\n",
    "# result_df = combined_rdd.toDF([\"TimeZone\", \"Cases\"]).toPandas()\n",
    "\n",
    "# faster dataframe functions\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by Timezone and count the occurrences\n",
    "result_df = (\n",
    "    df.groupBy(\"Timezone\")\n",
    "    .agg(F.count(\"*\").alias(\"Cases\"))  # Count the number of cases for each timezone\n",
    "    .toPandas()  # Convert to Pandas DataFrame\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ee4d57-9966-4c6e-a01c-67e38864348b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "u2VmA5haPUwa"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# import seaborn as sns\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Timezone\", y=\"Cases\", data=result_df, palette=\"viridis\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Timezone\", fontsize=12)\n",
    "plt.ylabel(\"Number of Accidents\", fontsize=12)\n",
    "plt.title(\"Number of Accidents by Timezone\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a601bbc-1bff-4c2d-8a21-2c08f93ef09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NgxKi2OONrUi"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69fbece7-01ae-4d1d-aee8-0fc893f35ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KhIjJRMNN1dG"
   },
   "source": [
    "# Severity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a4f06b-9827-4a17-823b-e15ab13ee904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HWTbE7iYIZhI"
   },
   "outputs": [],
   "source": [
    "rdd = df.rdd\n",
    "mapped_rdd = rdd.map(lambda x: (x.Severity, 1))\n",
    "# Use combineByKey for optimized aggregation\n",
    "\n",
    "combined_rdd = mapped_rdd.combineByKey(\n",
    "    lambda value: value,  # CreateCombiner: Initialize the count for a key\n",
    "    lambda acc, value: acc + value,  # MergeValue: Add one to the existing count\n",
    "    lambda acc1, acc2: acc1 + acc2  # MergeCombiners: Combine counts from different partitions\n",
    ")\n",
    "result_df = combined_rdd.toDF([\"Severity\", \"Cases\"]).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92f2b50-15b8-49b1-9688-3659557ee90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "IY3deJlhPBRH",
    "outputId": "d50c2313-6a6b-4a7b-b1b5-81b824c0e392"
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f695a2b4-8c02-4692-92ef-17d2f2fec984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "twoSbwG9OW3T",
    "outputId": "418ee41f-1fac-465f-d609-b368bed3d21b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))  # Set figure size\n",
    "plt.pie(\n",
    "    result_df[\"Cases\"],\n",
    "    labels=result_df[\"Severity\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=[\"#ff9999\", \"#66b3ff\", \"#99ff99\", \"#ffcc99\"]\n",
    ")\n",
    "plt.title(\"Distribution of Accident Severity\")\n",
    "plt.axis(\"equal\")  # Equal aspect ratio ensures the pie is drawn as a circle\n",
    "plt.show()  # Display the pie chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322dcd42-dff9-4612-adf4-ff5a49cf4a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vwhwoI0-TdSL"
   },
   "source": [
    "# Accident Duration Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9849a1-7f99-49d0-9e9f-c84df54d7aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "id": "ghPyNocEhcd1",
    "outputId": "d6600da2-6b49-4fd6-c77b-cabfc2293a91"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "from pyspark.sql.functions import unix_timestamp, col, count, round, sum as _sum\n",
    "\n",
    "# Step 1: Calculate the duration in seconds\n",
    "df_with_duration = df.\\\n",
    "withColumn(\"Duration_Seconds\", unix_timestamp(\"End_Time\") - unix_timestamp(\"Start_Time\"))\\\n",
    ".filter(col(\"Duration_Seconds\") > 0) \\\n",
    ".withColumn(\"Duration_Minutes\", round(col(\"Duration_Seconds\") / 60))\n",
    "\n",
    "duration_counts_df = df_with_duration.groupBy(\"Duration_Minutes\").agg(count(\"*\").alias(\"Count\"))\n",
    "\n",
    "total_counts = duration_counts_df.agg(_sum(\"Count\").alias(\"Total\")).collect()[0][\"Total\"]\n",
    "\n",
    "# Step 6: Add a \"Percentage\" column\n",
    "duration_counts_df = duration_counts_df.withColumn(\"Percentage\", (col(\"Count\") / total_counts) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71df955-cb3b-4bb6-b7f5-ee0160ce63be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dYNsv23Ed1SM"
   },
   "outputs": [],
   "source": [
    "top_10_durations_df = duration_counts_df.orderBy(col(\"Count\").desc()).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548d20e9-0f98-445f-8163-0ae1a9b33003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Rsrsj-Vbdyvq"
   },
   "outputs": [],
   "source": [
    "# plot the\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Duration_Minutes\", y=\"Percentage\", data=top_10_durations_df, palette=\"viridis\", edgecolor=\"black\")\n",
    "\n",
    "# Add titles and labels with custom styling\n",
    "plt.title(\"Top 10 Frequent Accident Durations in Minutes\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Duration (Minutes)\", fontsize=14)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=14)\n",
    "\n",
    "# Annotate each bar with its value\n",
    "for index, row in top_10_durations_df.iterrows():\n",
    "    x_pos = index\n",
    "    y_pos = row[\"Percentage\"]\n",
    "    plt.text(x=x_pos, y=y_pos + 0.5, s=f\"{row['Percentage']:.2f}%\",\n",
    "             ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5551df18-c1c5-4ab2-a61a-c72e80ec74bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zc4qZyB5TeO8"
   },
   "source": [
    "# Year Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ee58e9-59d1-4728-a065-72bde4ba07b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JNLjQeAfhf2-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_by_year = df.groupBy(year(\"Start_Time\")).count().toPandas()\n",
    "df_by_year = df_by_year.sort_values(by=\"year(Start_Time)\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2611eb2a-9ba5-4fe6-ba8b-c482981afa6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Y8KpiAYxhiIB",
    "outputId": "dbd8b5ea-dec8-47b9-f804-4d6935df8c9c"
   },
   "outputs": [],
   "source": [
    "df_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e25acc8-52f7-4e9d-982e-aa4dc126db04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "cA_B96aOhjdN",
    "outputId": "12ded770-070f-46b8-d2a5-69393776ada9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab75bbef-e2c5-499c-a7bb-f9dbb0285aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "l9yhVEPTsg-1"
   },
   "source": [
    "# Group by year and visualize by coordinate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b68498-e53d-4c2f-9898-575b206946ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZtKkkA7zsmHf"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "df_sampled = (\n",
    "    df.selectExpr(\"year(Start_Time) as Year\", \"Start_Lat\", \"Start_Lng\")\n",
    "    .sample(withReplacement=False, fraction=600000 / df.count())  # Sample approximately 50,000 rows\n",
    "    .toPandas()\n",
    "    .dropna()  # Remove rows with missing data\n",
    ")\n",
    "\n",
    "# Step 2: Filter data for each year and create heatmaps\n",
    "years = df_sampled[\"Year\"].unique()  # Get the unique years\n",
    "heatmaps = {}  # Dictionary to store heatmaps for each year\n",
    "\n",
    "for year in sorted(years):\n",
    "    # Filter data for the specific year\n",
    "    df_filtered = df_sampled[df_sampled[\"Year\"] == year]\n",
    "\n",
    "    # Skip years with no data\n",
    "    if df_filtered.empty:\n",
    "        continue\n",
    "\n",
    "    # Create a base map centered on the dataset\n",
    "    center_lat = df_filtered[\"Start_Lat\"].mean()\n",
    "    center_lng = df_filtered[\"Start_Lng\"].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lng], zoom_start=5)\n",
    "\n",
    "    # Prepare heatmap data for the year\n",
    "    heat_data = df_filtered[[\"Start_Lat\", \"Start_Lng\"]].values.tolist()\n",
    "\n",
    "    # Add heatmap layer\n",
    "    HeatMap(heat_data, radius=10, blur=15, max_zoom=1).add_to(m)\n",
    "\n",
    "    # Save the map to the dictionary for later access\n",
    "    heatmaps[year] = m\n",
    "\n",
    "    # Save the map as an HTML file for viewing\n",
    "    # m.save(f\"accidents_heatmap_{year}.html\")\n",
    "\n",
    "    # Display the map directly in the notebook\n",
    "    display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b75551-8605-4078-adb9-1967f5fa5a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "snSxrnv7Tgfr"
   },
   "source": [
    "# Year Analysis based on Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6deb3749-914f-4816-9ade-4a272aad5280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "id": "1Y58ooPUhpxn",
    "outputId": "bdcf1929-d937-4933-b358-ba4496e6a8d7"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Select the required columns\n",
    "df_selected = df.select(\"Severity\", \"Start_Time\")\n",
    "\n",
    "# Extract year from Start_Time and group by Severity and year\n",
    "df_grouped = df_selected.groupBy(\"Severity\", year(\"Start_Time\").alias(\"Year\")).count()\n",
    "\n",
    "# Convert to Pandas for inspection (optional)\n",
    "df_grouped_pandas = df_grouped.toPandas()\n",
    "df_grouped_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce42100-96a6-4284-90f9-0163a33ecfa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "id": "tVnNa5oAhqMt",
    "outputId": "14eac8ee-c314-48a9-91d5-2bf075ad4945"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_grouped_pandas is the grouped data in Pandas DataFrame format\n",
    "# Pivot the data to make years the index, severity classes the columns, and counts the values\n",
    "pivot_data = df_grouped_pandas.pivot(\n",
    "    index=\"Year\", columns=\"Severity\", values=\"count\"\n",
    ").fillna(0)\n",
    "\n",
    "# Sort the years for better visualization\n",
    "pivot_data = pivot_data.sort_index()\n",
    "\n",
    "# Calculate the total number of accidents per year\n",
    "pivot_data[\"Total\"] = pivot_data.sum(axis=1)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "severity_classes = pivot_data.columns[:-1]  # Exclude the \"Total\" column\n",
    "bottom = None  # Used for stacking bars\n",
    "\n",
    "# Plot each severity class as a segment of the stacked bar\n",
    "for severity in severity_classes:\n",
    "    if bottom is None:\n",
    "        bar = plt.bar(\n",
    "            pivot_data.index, pivot_data[severity], label=f\"Severity {severity}\"\n",
    "        )\n",
    "        bottom = pivot_data[severity]\n",
    "    else:\n",
    "        bar = plt.bar(\n",
    "            pivot_data.index,\n",
    "            pivot_data[severity],\n",
    "            bottom=bottom,\n",
    "            label=f\"Severity {severity}\",\n",
    "        )\n",
    "        bottom += pivot_data[severity]\n",
    "\n",
    "# Plot the line chart on top of the bar chart\n",
    "plt.plot(\n",
    "    pivot_data.index,\n",
    "    pivot_data[\"Total\"],\n",
    "    color=\"red\",\n",
    "    marker=\"o\",\n",
    "    label=\"Total Accidents\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title(\"Number of Accidents by Year and Severity\", fontsize=16)\n",
    "plt.xlabel(\"Year\", fontsize=14)\n",
    "plt.ylabel(\"Number of Accidents\", fontsize=14)\n",
    "plt.xticks(pivot_data.index, rotation=45)\n",
    "plt.legend(title=\"Severity\", fontsize=12)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c01e205-0e1f-488a-a838-3ba6c5f94a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wIYhYoONTmJt"
   },
   "source": [
    "# Month Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ab450a-09f5-443c-9dbc-07c777c74a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-escmMj5l27Z"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "df_by_month = df.groupBy(month(\"Start_Time\")).count().toPandas()\n",
    "df_by_month = df_by_month.sort_values(by=\"month(Start_Time)\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8c1d1fa-67f6-44a6-811d-26e5b639063b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "0UNmZnRQl4vy",
    "outputId": "69141898-a1a8-4f30-d3a7-810bb3d0ac37"
   },
   "outputs": [],
   "source": [
    "# show a bar chart of the number of accidents by month\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_by_month[\"month(Start_Time)\"], df_by_month[\"count\"])\n",
    "plt.title(\"Number of Accidents by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89b363e-94bc-48a4-9528-6964a087fb4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZWueb7MZl9JU"
   },
   "source": [
    "## Group by month + Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb2dce4-8258-48f4-9085-46f1fa06bc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_x3seYjWl-Zx"
   },
   "outputs": [],
   "source": [
    "# Extract relevant columns and convert to RDD\n",
    "rdd = df.select(\"Start_Time\", \"Severity\").rdd.map(\n",
    "    lambda row: (row[\"Start_Time\"].month, row[\"Severity\"])\n",
    ")  # Extract month and severity\n",
    "\n",
    "# Map to key-value pairs and count occurrences\n",
    "rdd_grouped = rdd.map(\n",
    "    lambda x: ((x[0], x[1]), 1)\n",
    ").reduceByKey(  # Key: (Month, Severity), Value: 1\n",
    "    lambda a, b: a + b\n",
    ")  # Aggregate counts\n",
    "\n",
    "# Convert to a more structured format for plotting\n",
    "data = rdd_grouped.map(lambda x: (x[0][0], x[0][1], x[1])).collect()\n",
    "df_grouped = pd.DataFrame(data, columns=[\"Month\", \"Severity\", \"Count\"])\n",
    "\n",
    "df_grouped = df_grouped.sort_values(by=\"Month\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294dd852-cb58-4b0a-a78e-c33bc4ed93c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "lEzUe0SZmEax",
    "outputId": "91a186f9-800e-4b06-8e43-8c0bc13d279a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_grouped is already created and sorted\n",
    "\n",
    "# Pivot the data to prepare for plotting\n",
    "# Rows: Months (x-axis), Columns: Severity levels, Values: Count\n",
    "df_pivot = df_grouped.pivot(index=\"Month\", columns=\"Severity\", values=\"Count\").fillna(0)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "bottom = None\n",
    "\n",
    "# Plot each severity level as a segment of the stacked bar\n",
    "for severity in df_pivot.columns:\n",
    "    if bottom is None:\n",
    "        plt.bar(\n",
    "            df_pivot.index, df_pivot[severity], label=f\"Severity {severity}\"\n",
    "        )  # First layer\n",
    "        bottom = df_pivot[severity]\n",
    "    else:\n",
    "        plt.bar(\n",
    "            df_pivot.index,\n",
    "            df_pivot[severity],\n",
    "            bottom=bottom,\n",
    "            label=f\"Severity {severity}\",\n",
    "        )  # Subsequent layers\n",
    "        bottom += df_pivot[severity]\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title(\"Number of Accidents by Month and Severity\", fontsize=16)\n",
    "plt.xlabel(\"Month\", fontsize=14)\n",
    "plt.ylabel(\"Number of Accidents\", fontsize=14)\n",
    "plt.xticks(ticks=df_pivot.index, labels=df_pivot.index, rotation=45)\n",
    "plt.legend(title=\"Severity\", fontsize=12, loc=\"upper right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea7a007-a860-4441-b773-91a40bf4e036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tK09J0xwTm-n"
   },
   "source": [
    "# Day Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adcd61f3-4bb9-40c9-8536-d617bc9182e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esjMV4XXuluq",
    "outputId": "6a0dadd0-385d-4313-ccb7-89f54955f9e4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Extract the day of the week from Start_Time\n",
    "df_with_day = df.withColumn(\"Day_of_Week\", date_format(\"Start_Time\", \"EEEE\"))\n",
    "\n",
    "# Group by the day of the week and count the number of accidents\n",
    "df_grouped_by_day = df_with_day.groupBy(\"Day_of_Week\").count()\n",
    "\n",
    "# Sort by day of the week in the correct order (Monday, Tuesday, ..., Sunday)\n",
    "day_order = [\n",
    "    \"Monday\",\n",
    "    \"Tuesday\",\n",
    "    \"Wednesday\",\n",
    "    \"Thursday\",\n",
    "    \"Friday\",\n",
    "    \"Saturday\",\n",
    "    \"Sunday\",\n",
    "]\n",
    "df_grouped_by_day = df_grouped_by_day.toPandas()\n",
    "df_grouped_by_day[\"Day_of_Week\"] = pd.Categorical(\n",
    "    df_grouped_by_day[\"Day_of_Week\"], categories=day_order, ordered=True\n",
    ")\n",
    "df_grouped_by_day = df_grouped_by_day.sort_values(by=\"Day_of_Week\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(df_grouped_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67005f4c-4a8c-4617-963d-a670dd81b40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "48_WUSgWumff",
    "outputId": "ed633637-984c-41d4-a8e6-a5978c67c7bf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_grouped_by_day[\"Day_of_Week\"], df_grouped_by_day[\"count\"], color=\"skyblue\")\n",
    "plt.title(\"Number of Accidents by Day of the Week\", fontsize=16)\n",
    "plt.xlabel(\"Day of the Week\", fontsize=14)\n",
    "plt.ylabel(\"Number of Accidents\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78101678-2ae8-4f6e-a241-b95c6b6dd3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4rVCd_1bTpiC"
   },
   "source": [
    "# Hour Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5bf67e0-0140-4c75-bd13-b61cbb0a05d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "creates an enhanced bar chart to visualize the number of accidents by hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbdc9d3-e55f-4bb0-999b-b657c96f0288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gqEpDKCfwV2Y"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the dataframe to an RDD and extract the hour from Start_Time\n",
    "rdd = df.select(\"Start_Time\").rdd.map(lambda row: row[\"Start_Time\"].hour)\n",
    "\n",
    "# Map each hour to a key-value pair and count occurrences using reduceByKey\n",
    "rdd_grouped = rdd.map(lambda hour: (hour, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect the results and convert to Pandas DataFrame for plotting\n",
    "data = rdd_grouped.collect()  # Collect the results as a list of tuples\n",
    "df_grouped_by_hour = pd.DataFrame(\n",
    "    data, columns=[\"Hour\", \"Count\"]\n",
    ")  # Convert to Pandas DataFrame\n",
    "\n",
    "# Sort the DataFrame by hour\n",
    "df_grouped_by_hour = df_grouped_by_hour.sort_values(by=\"Hour\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801467c0-e2e0-4911-a270-a46410ed248d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "NrV-RDBbwWsR",
    "outputId": "08293205-e021-4a4f-9eeb-9deb7bbc4d2c"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Step 5: Plot the enhanced bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(\n",
    "    x=\"Hour\",\n",
    "    y=\"Count\",\n",
    "    data=df_grouped_by_hour,\n",
    "    palette=\"viridis\",  # Use a visually appealing color palette\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "# Add titles and labels with custom styling\n",
    "plt.title(\n",
    "    \"Number of Accidents by Hour of the Day\", fontsize=20, fontweight=\"bold\", pad=20\n",
    ")\n",
    "plt.xlabel(\"Hour of the Day (24-hour format)\", fontsize=16, labelpad=10)\n",
    "plt.ylabel(\"Number of Accidents\", fontsize=16, labelpad=10)\n",
    "\n",
    "# Format x-ticks and y-ticks\n",
    "plt.xticks(ticks=range(0, 24), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add gridlines for better readability\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", color=\"gray\", alpha=0.7)\n",
    "\n",
    "# Annotate each bar with its value\n",
    "for index, row in df_grouped_by_hour.iterrows():\n",
    "    plt.text(\n",
    "        x=row[\"Hour\"],\n",
    "        y=row[\"Count\"]\n",
    "        + max(df_grouped_by_hour[\"Count\"]) * 0.01,  # Position slightly above the bar\n",
    "        s=f\"{int(row['Count'])}\",  # Display the count value\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=11,\n",
    "        color=\"black\",\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Adjust layout for better fitting\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af12b16d-309d-4850-8f24-d69b118a1a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "nalyzes accident severity distribution by hour intervals, counting occurrences for each severity level and visualizing the results as pie charts for each hour interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04da277d-9358-488a-ab52-d6cec3363639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "uX7Y28L1JUlz",
    "outputId": "89ba0236-2ba6-487d-9406-cdb6f768a094"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "# Extract the hour and severity from the dataframe and convert to RDD\n",
    "rdd = df.select(\"Start_Time\", \"Severity\").rdd.map(lambda row: (row[\"Start_Time\"].hour, row[\"Severity\"]))\n",
    "\n",
    "# Map to key-value pairs ((hour_interval, severity), 1) and count occurrences\n",
    "# Define hour intervals (e.g., 0-3, 4-7, 8-11, 12-15, 16-19, 20-23)\n",
    "def get_hour_interval(hour):\n",
    "    if 0 <= hour <= 3:\n",
    "        return \"0-3\"\n",
    "    elif 4 <= hour <= 7:\n",
    "        return \"4-7\"\n",
    "    elif 8 <= hour <= 11:\n",
    "        return \"8-11\"\n",
    "    elif 12 <= hour <= 15:\n",
    "        return \"12-15\"\n",
    "    elif 16 <= hour <= 19:\n",
    "        return \"16-19\"\n",
    "    else:\n",
    "        return \"20-23\"\n",
    "\n",
    "rdd_grouped = rdd.map(lambda x: ((get_hour_interval(x[0]), x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Convert to a more structured format for plotting\n",
    "data = rdd_grouped.map(lambda x: (x[0][0], x[0][1], x[1])).collect()\n",
    "df_grouped = pd.DataFrame(data, columns=[\"Hour_Interval\", \"Severity\", \"Count\"])\n",
    "\n",
    "# Aggregate counts by severity\n",
    "severity_counts = df_grouped.groupby(\"Severity\")[\"Count\"].sum().reset_index()\n",
    "\n",
    "# Plot pie charts for each hour interval\n",
    "hour_intervals = df_grouped[\"Hour_Interval\"].unique()\n",
    "severity_colors = {1: \"#ff9999\", 2: \"#66b3ff\", 3: \"#99ff99\", 4: \"#ffcc99\"}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, interval in enumerate(sorted(hour_intervals)):\n",
    "    interval_data = df_grouped[df_grouped[\"Hour_Interval\"] == interval]\n",
    "    severity_counts = interval_data.groupby(\"Severity\")[\"Count\"].sum().reset_index()\n",
    "\n",
    "    colors = [severity_colors[sev] for sev in severity_counts[\"Severity\"]]\n",
    "\n",
    "    axes[i].pie(\n",
    "        severity_counts[\"Count\"],\n",
    "        labels=[f\"Severity {int(sev)}\" for sev in severity_counts[\"Severity\"]],\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        colors=colors\n",
    "    )\n",
    "    axes[i].set_title(f\"Hour Interval {interval}\")\n",
    "\n",
    "plt.suptitle(\"Distribution of Accident Severity by Hour Interval\", fontsize=20, fontweight=\"bold\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14357bf6-367f-44eb-98fd-b7ba57aaeb83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vzDy36X7_Hj6"
   },
   "source": [
    "# Road Condition Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937e6858-3b50-4d15-96cc-15eb18e7fb59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "analyzes various road conditions related to accidents, visualizing the top 8 road conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56715c67-cfdf-4689-88ac-6e9c41febfe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "QSNdbQo89Qh4",
    "outputId": "90d2716b-e286-4681-a1f3-e00702a22379"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Extract relevant columns\n",
    "road_conditions = [\n",
    "    \"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\", \"No_Exit\",\n",
    "    \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Traffic_Calming\",\n",
    "    \"Traffic_Signal\", \"Turning_Loop\"\n",
    "]\n",
    "\n",
    "# Step 2: Map each condition to a key-value pair and count occurrences using reduceByKey\n",
    "condition_counts_rdd = df.select(road_conditions).rdd.flatMap(\n",
    "    lambda row: [(condition, 1) for condition in road_conditions if row[condition]]\n",
    ").reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 3: Collect the results and convert to Pandas DataFrame\n",
    "condition_counts = condition_counts_rdd.collect()\n",
    "condition_df = pd.DataFrame(condition_counts, columns=[\"Condition\", \"Count\"])\n",
    "\n",
    "# Step 4: Calculate the total number of accidents for percentage calculation\n",
    "total_accidents = df.count()\n",
    "condition_df[\"Percentage\"] = (condition_df[\"Count\"] / total_accidents) * 100\n",
    "\n",
    "# Step 5: Sort by percentage and select the top 8 conditions\n",
    "top_conditions_df = condition_df.sort_values(by=\"Percentage\", ascending=False).head(8)\n",
    "\n",
    "# Step 6: Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x=\"Percentage\", y=\"Condition\", data=top_conditions_df,\n",
    "    palette=sns.color_palette(\"rainbow\", n_colors=8)\n",
    ")\n",
    "plt.xlabel(\"Percentage of Accidents\")\n",
    "plt.title(\"Top 8 Road Conditions by Percentage of Accidents\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c5294b-a81a-4f2b-ac4b-5d4995bc30b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "analyzes road conditions related to accidents, counting occurrences by severity, and visualizes the severity distribution for the top 5 road conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3b0e2f-c269-4771-a205-b1b385475171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "4WdXkztuCSo1",
    "outputId": "42aae113-8404-4f7f-c652-6a4572e5e5c8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Extract relevant columns and convert to RDD\n",
    "road_conditions = [\n",
    "    \"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\", \"No_Exit\",\n",
    "    \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Traffic_Calming\",\n",
    "    \"Traffic_Signal\", \"Turning_Loop\"\n",
    "]\n",
    "\n",
    "# Create a list of tuples (condition, severity) for each row\n",
    "rdd = df.select(road_conditions + [\"Severity\"]).rdd.flatMap(\n",
    "    lambda row: [(condition, row[\"Severity\"]) for condition in road_conditions if row[condition]]\n",
    ")\n",
    "\n",
    "# Step 2: Map to key-value pairs and count occurrences\n",
    "rdd_grouped = rdd.map(lambda x: ((x[0], x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 3: Convert to a more structured format for plotting\n",
    "data = rdd_grouped.map(lambda x: (x[0][0], x[0][1], x[1])).collect()\n",
    "df_grouped = pd.DataFrame(data, columns=[\"Condition\", \"Severity\", \"Count\"])\n",
    "\n",
    "# Step 4: Filter for the top 5 road conditions with the most accidents\n",
    "top_5_conditions = df_grouped.groupby(\"Condition\")[\"Count\"].sum().nlargest(5).index\n",
    "df_top_5 = df_grouped[df_grouped[\"Condition\"].isin(top_5_conditions)]\n",
    "\n",
    "# Define a color map for severities\n",
    "severity_colors = {\n",
    "    1: \"#ff9999\",  # Red\n",
    "    2: \"#66b3ff\",  # Blue\n",
    "    3: \"#99ff99\",  # Green\n",
    "    4: \"#ffcc99\"   # Orange\n",
    "}\n",
    "\n",
    "# Step 5: Plot pie charts for each of the top 5 road conditions\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "for i, condition in enumerate(top_5_conditions):\n",
    "    condition_data = df_top_5[df_top_5[\"Condition\"] == condition]\n",
    "    axes[i].pie(\n",
    "        condition_data[\"Count\"],\n",
    "        labels=[f\"Severity {sev}\" for sev in condition_data[\"Severity\"]],\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=140,\n",
    "        colors=[severity_colors[sev] for sev in condition_data[\"Severity\"]]\n",
    "    )\n",
    "    axes[i].set_title(condition)\n",
    "\n",
    "plt.suptitle(\"Accident Severity Distribution for Top 5 Road Conditions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "971d5241-7f70-48ed-b288-337009d11985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ApRdrDff_yvF"
   },
   "source": [
    "# Weather Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72be4d38-e92c-427a-8d4f-efdf55682d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "analyzes and visualizes the distribution of accidents by weather condition, displaying the top 10 conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36150b36-42cd-416f-bae2-99337a6c6c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UEnqrTjJCefF",
    "outputId": "589226e0-81ef-462f-a1df-794e577f6f91"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the Weather_Condition column and filter out None values\n",
    "weather_conditions_df = df.filter(col(\"Weather_Condition\").isNotNull())\n",
    "\n",
    "# Group by each weather condition and count the number of accidents\n",
    "weather_condition_counts = weather_conditions_df.groupBy(\"Weather_Condition\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Limit to top 10 weather conditions\n",
    "top_10_weather_conditions = weather_condition_counts.limit(10)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "weather_condition_df = top_10_weather_conditions.toPandas()\n",
    "\n",
    "# Calculate the percentage of accidents for each weather condition\n",
    "total_accidents = weather_conditions_df.count()\n",
    "weather_condition_df[\"Percentage\"] = (weather_condition_df[\"count\"] / total_accidents) * 100\n",
    "\n",
    "# Plot pie charts to visualize the distribution\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.pie(weather_condition_df[\"count\"], labels=weather_condition_df[\"Weather_Condition\"], autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)\n",
    "plt.title(\"Distribution of Accidents by Weather Condition (Top 10)\")\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "# Provide additional statistics\n",
    "print(\"Total number of accidents:\", total_accidents)\n",
    "print(\"\\nAccidents by Weather Condition (Top 10):\")\n",
    "print(weather_condition_df)\n",
    "\n",
    "# Additional bar chart for better visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(weather_condition_df[\"Weather_Condition\"], weather_condition_df[\"count\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Number of Accidents\")\n",
    "plt.title(\"Number of Accidents by Weather Condition (Top 10)\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the condition with the most accidents on top\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02aa793-9530-4a5a-ac1a-a5a3cb283832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "analyzes weather data by calculating and visualizing the top 5 percentage intervals for Temperature, Humidity, Pressure, Wind_Chill, Wind_Speed and Visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deed17b4-d8cd-490a-a2f3-bcfaa10c49f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tHh9pfNO1SLm",
    "outputId": "2f494aab-28cf-4dad-a85a-177b6e1b932e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the columns to analyze\n",
    "columns = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Wind_Chill(F)\", \"Wind_Speed(mph)\", \"Visibility(mi)\"]\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "intervals_dict = {}\n",
    "\n",
    "# Function to find top 5 intervals with highest percentage\n",
    "def find_top_intervals(df, column):\n",
    "    try:\n",
    "        # Round to nearest 10 and group by intervals\n",
    "        df_intervals = df.select(column).filter(F.col(column).isNotNull()).withColumn(\n",
    "            \"Interval\", (F.round(F.col(column) / 10) * 10)\n",
    "        ).groupBy(\"Interval\").agg(\n",
    "            F.count(\"*\").alias(\"Count\")\n",
    "        )\n",
    "\n",
    "        # Calculate total count\n",
    "        total_count = df_intervals.agg(F.sum(\"Count\")).collect()[0][0]\n",
    "\n",
    "        # Calculate percentage\n",
    "        if total_count > 0:\n",
    "            df_intervals = df_intervals.withColumn(\"Percentage\", (F.col(\"Count\") / total_count) * 100)\n",
    "        else:\n",
    "            df_intervals = df_intervals.withColumn(\"Percentage\", F.lit(0))\n",
    "\n",
    "        # Get top 5 intervals\n",
    "        top_intervals = df_intervals.orderBy(F.col(\"Percentage\").desc()).limit(5).toPandas()\n",
    "        return top_intervals\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing column {column}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Interval\", \"Count\", \"Percentage\"])\n",
    "\n",
    "# Analyze each column and store the results\n",
    "for column in columns:\n",
    "    intervals_dict[column] = find_top_intervals(df, column)\n",
    "\n",
    "# Plot the results\n",
    "for column, data in intervals_dict.items():\n",
    "    if not data.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=\"Interval\", y=\"Percentage\", data=data, palette=\"viridis\", edgecolor=\"black\")\n",
    "        plt.title(f\"Top 5 Intervals with Highest Percentage for {column}\", fontsize=16)\n",
    "        plt.xlabel(\"Interval\", fontsize=14)\n",
    "        plt.ylabel(\"Percentage (%)\", fontsize=14)\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f3a3f6-fb37-4767-93b2-0eef18cd1912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "processes accident data by weather conditions and severity, counting occurrences and visualizing the top 10 conditions as pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47fc609-c4fe-4081-a881-f8585e5d5cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F7f-QitjCMyU",
    "outputId": "fab447b8-1cda-41e7-bc4f-b100425d9dec"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract relevant columns and convert to RDD\n",
    "weather_conditions = df.select(\"Weather_Condition\", \"Severity\").filter(col(\"Weather_Condition\").isNotNull()).rdd\n",
    "\n",
    "# Create a list of tuples (weather_condition, severity) for each row\n",
    "rdd = weather_conditions.map(lambda row: (row[\"Weather_Condition\"], row[\"Severity\"]))\n",
    "\n",
    "# Map to key-value pairs and count occurrences\n",
    "rdd_grouped = rdd.map(lambda x: ((x[0], x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Convert to a more structured format for plotting\n",
    "data = rdd_grouped.map(lambda x: (x[0][0], x[0][1], x[1])).collect()\n",
    "df_grouped = pd.DataFrame(data, columns=[\"Weather_Condition\", \"Severity\", \"Count\"])\n",
    "\n",
    "# Keep only the top 10 weather conditions by total count\n",
    "top_10_weather_conditions = df_grouped.groupby(\"Weather_Condition\")[\"Count\"].sum().nlargest(10).index\n",
    "df_grouped = df_grouped[df_grouped[\"Weather_Condition\"].isin(top_10_weather_conditions)]\n",
    "\n",
    "# Define a consistent color mapping for severities\n",
    "severity_colors = {\n",
    "    1: \"#ff9999\",  # Red\n",
    "    2: \"#66b3ff\",  # Blue\n",
    "    3: \"#99ff99\",  # Green\n",
    "    4: \"#ffcc99\"   # Orange\n",
    "}\n",
    "\n",
    "# Plot pie charts for each weather condition\n",
    "fig, axes = plt.subplots(5, 2, figsize=(20, 25))\n",
    "fig.suptitle(\"Accidents by Weather Condition and Severity (Top 10)\", fontsize=20)\n",
    "\n",
    "# Plot each weather condition as a separate pie chart\n",
    "for i, weather_condition in enumerate(top_10_weather_conditions):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    data = df_grouped[df_grouped[\"Weather_Condition\"] == weather_condition]\n",
    "    labels = [f\"Severity {severity}\" for severity in data[\"Severity\"]]\n",
    "    colors = [severity_colors[severity] for severity in data[\"Severity\"]]\n",
    "    ax.pie(\n",
    "        data[\"Count\"],\n",
    "        labels=labels,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=140,\n",
    "        colors=colors,\n",
    "    )\n",
    "    ax.set_title(weather_condition, fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "US_accidents_analysis",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
